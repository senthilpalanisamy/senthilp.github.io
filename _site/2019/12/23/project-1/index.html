<!DOCTYPE html>
<html>
    <!-- Mathjax Support -->
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Baxter, the lego-builder</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Portfolio of a robotics Computer Vision Engineer">
    <meta name="keywords" content="Robotics, Computer Vision, Portfolio, jobs, Phd" />
    <meta name="author" content="Palanisamy Senthil">
    <link rel="canonical" href="http://localhost:4000/2019/12/23/project-1/">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">

    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="/style.css">

    <!-- Google verification -->
    

    <!-- Bing Verification -->
    

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css">
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

    <body id="page-top" class="index">
       <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#page-top">Senthil Palanisamy</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li class="page-scroll">
                        <a href="#portfolio">Portfolio</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#about">About</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#endorsements">Endorsements</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#contact">Message me</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#page-footer">Contact Details</a>
                    </li>
                    <li class="page-scroll">
                        <a href="files/resume.pdf" target="_blank">Resume</a>
                    </li>
                    <li class="page-scroll">
                        <a href="https://www.linkedin.com/in/senthil-palanisamy/" target="_blank">Linkedin</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

        <!-- Header -->
    <header>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <img class="img-responsive" src="img/profile.png" alt="">
                    <div class="intro-text">
                        <span class="name">Senthil Palanisamy</span>
                        <hr class="star-light">
                        <span class="skills">The power of human eyes and human brain can never be appreciated fully without trying to power a robot's vision. The intersection of robotics and computer vision, to me, is the most fascinating place of all</span>
                    </div>
                </div>
                </div>
            </div>
        </div>
    </header>

    
    <!-- Portfolio Grid Section -->
    <section id="portfolio">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Portfolio</h2>
                    <hr class="star-primary">
                </div>
            </div>
            <div class="row">
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-1" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/baxterbuilder.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Baxter, the lego-builder</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-2" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/astar.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>A PID controller with an online AStar planner</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-3" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/youbot.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Object Manipulation using Youbot in Simulation.</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-4" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/ocv.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>OCV (Optical Character Verification) using smart camera</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-5" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/seatbelt.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Seat Belt Detection</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-6" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/card_reader.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Aadhar Card reader</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-7" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/magic_book.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Magic book</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-8" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/rrt.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Rapidly Exploring Random Tree</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-9" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/balltracker.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Servo Controlled Ball tracking</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-10" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/computer_vision.jpg" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Other Computer Vision projects</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-11" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/workshop.jpg" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Workshops Conducted</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-12" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <img src="img/portfolio/windmill_impact.gif" class="img-responsive" alt="image-alt">

                           <div class="inner-text">                                           
                             <h4>Dynamic simulation of a windmill impact</h4>                                     
                          </div>                                                             
                          </a>
                    </div>
                
            </div>
        </div>
    </section>

     <!-- About Section -->
    <section class="success" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>About</h2>
                    <hr class="star-light">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 ">
                    <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <img src="img/me_casual.jpg" width="58%" align="middle"><h3>Brief Overview </h3> I am Senthil Palanisamy. I was born and brought up in India. I moved to US in August 2019 to begin my Masters in Robotics at Northwestern. My undergraduation was in electronics and then I worked for about 3 years in computer vision. Right now, I get excited about solving robotics problems involving computer vision. 
                </div>
                <div class="col-lg-4">
	                <h3>Education </h3> <ul><li>Bachelor of Engineering in Electronics & Communication - Anna University (Graduated May 2016.)</li><li>Master of Science in Robotics - Northwestern University (Expected Graduation Dec 2020)</li></ul>
                    <p><h3>Experience: 3 Years </h3> <ul><li> TartanSense | Robotics Engineer - (March - July 2019)</li><li> Soliton Technologies | Computer Vision R&D Engineer(May 2016- Feb 2019)</li></ul> For more information about this experience please see my resume (button below).<br></p>
                </div>
                <div class="col-lg-4">
                  <p><h3>Courses taken </h3> The courses I have taken include<span>&#58;</span> Embedded Systems, Robotic Manipulation, Machine Dynamics, ML and AI for robotics<br><br> The online courses I have completed include Stanford CS231n- Convolutional Neural Networks for Visual Recognition, Stanford CS229- Machine Learning, MIT 18.06- Linear Algebra, MIT 6.041 - Probabilistic Systems Aand Applied Probability, MIT 6.006 - Introduction to Algorithms<br><br><h3>Publications based on undergraduate work</h3><ul><li><a href="https://ieeexplore.ieee.org/document/8711348" target="_blank" style="color:rgb(0,0,255);" >Automated Robotic Moisture Monitoring in Agricultural Fields(My final year thesis)</a></li><li><a href="https://ieeexplore.ieee.org/document/8711028" target="_blank" style="color:rgb(0,0,255);" >Smart Mobile Phone Usage Restriction by Extending Phone Circuitry — An Alternative to Jamming(Third year thesis)</a></li></ul>
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <a href="files/resume.pdf" target="_blank" class="btn btn-lg btn-outline">
                        <i class="Resume-download"></i> Download Resume
                    </a>
                </div>
            </div>
        </div>
    </section>

    
 <!-- Endorsements Section -->
    <section class="success" id="endorsements">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Endorsements</h2>
                    <hr class="star-light">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 ">
                    <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <img src="img/portfolio/Jai.jpeg" width="58%" align="middle"><h4>From Jaisimha Rao, Founder/ CEO of TartanSense ( I worked in TartanSense for 4 months ) </h4> Senthil joined TartanSense at the product development stage where he displayed a set of skills that is rare amongst Robotics Engineers.<br><br> Senthil's "superpower" is his ability to cut through all the noise and make thorough decisions with clarity. This is indeed a gift for any organization that has several stakeholders with strong opinions. Senthil's performance in such a setting where his, firm but polite manner of communicating technical decisions made him shine in several crucial product development conversations.<br><br> Senthil subsequently backed up his decisions by building our data collection robot and our computer vision parameters. Once again his detail-oriented approach of calibrating the camera and working through all combinations helped us avoid any costly delays.<br><br> In short, Senthil's strong technical fundamentals combined with his brisk and cheerful demeanor in being a team leader are a set of combined skills that make him valuable to any organization.
                </div>

            <div class="row">
                <div class="col-lg-4 ">
                    <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <img src="img/portfolio/Sumod.jpeg" width="58%" align="middle"><h4>From Sumod Mohan, Founder Autoinfer, 10+ years experience in Computer Vision  ( I worked under Sumod for 2.7 years) </h4>Senthil was part of the CVD Group at Soliton and made significant contributions in Algorithm Development, Prototype Development, and Product Development for various projects involving Computer Vision Problems such as Object Detection/Recognition, Segmentation and certain Geometric Algorithms. He stood out in his desire to understand problems at its core, to make significant deep technical contributions with their solutions and his discipline to cement his knowledge (as evidenced by the online courses he completed). I am sure he would excel at Research and/or Development in both Academia and Industry.<br><br> He is extremely open and promotes openness and that made it very easy to work with him. His enthusiasm for ultra marathons and his desire to make an impact on society also was very apparent and also energized a lot of other Solitons.
                </div>

            <div class="row">
                <div class="col-lg-4 ">
                    <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <align="middle"><br><br><br><br><br><br><br><br><br><br><br><br><br><br>Endorsements quoted from Linkedin without modifications.<br> Link to <a href="https://www.linkedin.com/in/sumod-k-mohan-3a30127/" target="_blank" style="color:rgb(0,0,255);"> Sumod's</a> Profile.<br><br> Link to <a href="https://www.linkedin.com/in/jaisimha-rao-we-re-hiring-b37a466b/" target="_blank" style="color:rgb(0,0,255);">Jai's</a>Profile.
                </div>
            </div>
        </div>
    </section>


    
    <!-- Contact Section -->
<section id="contact">
	<div class="container">
		<div class="row">
			<div class="col-lg-12 text-center">
				<h2>Message Me</h2>
				<hr class="star-primary">
			</div>
		</div>
		<div class="row">
			<div class="col-lg-8 col-lg-offset-2">
				<form  action="//formspree.io/senthillihtnes1994@gmail.com" method="POST" name="sentMessage" id="contactForm" novalidate>
					<div class="row control-group">
						<div class="form-group col-xs-12 floating-label-form-group controls">
							<label for="name">Name</label>
							<input type="text" name="name" class="form-control" placeholder="Name" id="name" required data-validation-required-message="Please enter your name.">
							<p class="help-block text-danger"></p>
						</div>
					</div>
					<div class="row control-group">
						<div class="form-group col-xs-12 floating-label-form-group controls">
							<label for="email">Email Address</label>
							<input type="email" name="_replyto" class="form-control" placeholder="Email Address" id="email" required data-validation-required-message="Please enter your email address.">
							<p class="help-block text-danger"></p>
						</div>
					</div>
					<div>
						<input type="hidden"  name="_subject" value="New submission!">
						<input type="text" name="_gotcha" style="display:none" />
					</div>
					<div class="row control-group">
						<div class="form-group col-xs-12 floating-label-form-group controls">
							<label for="message">Message</label>
							<textarea rows="5" name="message" class="form-control" placeholder="Message" id="message" required data-validation-required-message="Please enter a message."></textarea>
							<p class="help-block text-danger"></p>
						</div>
					</div>
					<br>
					<div id="success"></div>
					<div class="row">
						<div class="form-group col-xs-12">
							<button type="submit" class="btn btn-success btn-lg">Send</button>
						</div>
					</div>
				</form>
			</div>
		</div>
	</div>
</section>

    

        <!-- Footer -->
    <section id="page-footer">
    <footer class="text-center">
        <div class="footer-above">
            <div class="container">
                <div class="row">
                    <div class="footer-col col-md-4">
                        <h3>Location</h3>
                        <p>
                            
                                1241 Emerson Avenue, Apt-2 <br>
		                    
                                Illinois, 60201 <br>
		                    
                        </p>
                    </div>
                    <div class="footer-col col-md-4">
                        <h3>Around the Web</h3>
                        <ul class="list-inline">
                            
                            <li>
                                <a href="https://github.com/senthilpalanisamy" target="_blank" class="btn-social btn-outline"><i class="fa fa-fw fa-github"></i></a>
                            </li>
		                    
                            <li>
                                <a href="https://www.linkedin.com/in/senthil-palanisamy/" target="_blank" class="btn-social btn-outline"><i class="fa fa-fw fa-linkedin"></i></a>
                            </li>
		                    
                        </ul>
                    </div>
                    <div class="footer-col col-md-4">
                        <h3>Contact</h3>
                        <p>
                            
                                (872) 985-1814 <br>
                            
                                senthilpalanisamy2020@u.northwestern.edu <br>
                            
                        </p>
                    </div>
                    <div class="footer-col col-md-4">
                        <h3></h3>
                        <p></p>
                    </div>
                </div>
            </div>
        </div>
        <div class="footer-below">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        Copyright &copy; Senthil Palanisamy 2020
                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-top page-scroll visible-xs visible-sm">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>
    </section>

     <!-- Portfolio Modals -->
 
    <div class="portfolio-modal modal fade" id="portfolioModal-1" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Baxter, the lego-builder</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/baxterbuilder.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2>My contribution in this team project</h2> I implemented the whole node for computer vision, which detects AR blocks, red lego blocks and does inverse projection to find the 3D location of a point on a known plane corresponding to a pixel on the camera. I also setup the RoS pipeline for the whole project and integrated my node into our RoS pipeline <br><br><h2> Overview </h2> For this project, Baxter assembles a MEGA BLOKS pyramid from the blocks provided by the user. The flow is:<ul><li>The Blocks that can be picked up are detected on Baxter's camera image using OpenCV</li><li>Among all detected blocks, a block is chosen at random and its 3D location is estimated</li><li>The block is picked up and moved to the drop location using position control in Moveit</li><li> The block is pressed against the plate by using force control in Moveit and finally, as a double check, the gripper hand is pushed against the block to ensure that the block is pushed in.</ul><br><br><h2>detailed explanation</h2> <br><br><h4>computer vision</h4>We used the Baxter's right hand to observe the base plate, spot a red brick and determine its position for pickup. First to know the plane in which the base plate is located, all AR frames visible in the camera's field of view are detected. Then the pre-calibrated intrinsic camera parameters are used to find inverse projection ray corresponding to a pixel on the camera's sensor. Then the 3D location corresponding to a pixel is calculated by finding the intersection of the inverse projected ray and ground plane defined by one of AR tags. The 3D point corresponding to the pixel is determined in multiple AR frames and then these points are converted to the Baxter world frame representation. As a result, we get multiple points in world frame that correspond to the same pixel in Baxter's camera image but obtained using different AR frames as reference. The median of all those points is found out and that point is taken to be the true 3D location corresponding to the pixel in the image. Reason why we use multiple AR frames when one should be sufficient in theory - Sometimes AR tags are affected by lighting and their 3D pose with respect to the camera oscillates. Even worse sometimes, they are not detected. Using multiple AR tags this way increases both the roboustness and the precision of the system. We were able to locate points within +/- 1 cm accuracy through this process.<br><br> <h4>Pickup and Placning</h4>We then use move it to move Baxter's left arm to the block pickup location, pickup the block by controlling gripper through Baxter's native libraries and then use move it again to move the blocks to a pre-configured goal location. Position control was used to move Baxter's arm to pickup and drop locations. We had standoff points before the pickup and drop, that were designed to be a few cms above the actual pickup and drop location. When placing the block, we used force control to press the block against the plate. The whole process is repeated until 3 blocks are placed to construct a small pyramid. A video showing this whole process in action is shown below but it should be noted that the video has been sped up 3x for viewing convenience <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/mz1FwBR94og" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br> If you want to know more on the project, please check out the <a href="https://github.com/senthilpalanisamy/final-project-megabloks" target="_blank">code</a> in github.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-2" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>A PID controller with an online AStar planner</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/astar.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2>Brief Description</h2><br><br> <h4>A star planning</h4> The A star algorithm uses a heuristic along with the node transition cost to evaluate the cost of each neighboring node. All the neighboring nodes of the current node are added to the open list and the node that the robot has currently visited is added to the closed list. In each step of the algorithm, the cheapest node from the open list is picked up and all its neighbors are added to the open list, after ensuring that those neighbors have not been added to the closed list. The termination point of the algorithm is when the goal node is added to the closed list. A star algorithm is both complete and optimal because it will always find a path to the goal (if it exists) and it finds the cheapest path that is available. There were two version of Astar implemented in this work. <ul><li> <h6> Offline version</h6> The term offline indicates that the map for navigation is known before hand. Therefore, a complete path is planned before taking the first step. Then the planned path is executed by using the PID controller. I designed my own heuristic function for this assignment, which calculates the distance between two nodes by splitting the distance into two parts, first a diagonal distance part and then, an axis aligned distance ( horizontal or vertical). The heuristic function that I came up with for this assignment is given below. <br> $$\bar{h}(n) = min({X_{diff}, Y_{diff}}) + | {X_{diff} - Y_{diff}} | $$ where \(X_{diff}\) is the difference in x-coordinates between the goal node and the current node and \(Y_{diff}\) is the difference in y coordinates between the goal node and the current node. The primary intuition behind this heuristic is that it tries to maximize the diagonal distance while trying to calculate the split distance. Since as per the problem description in the assignment, all 8 connected neighbors of a node have uniform costs, this heuristic is the best choice for this problem especially since this heuristic cost equals the true cost in an obstacle free world. The proof of admissibility of this heuristic and more intuition about it is presented in the report. </li> <li><h6> Online version</h6> The term online indicates that a map is not known before hand and only the nodes that are neighboring to the current node are known and the rest of the map is unexplored. In such scenarios, where backtracking is prohibited or extremely costly, the heuristic designed in the offline version needed modification to make more intelligent decision especially with respect to discriminating multiple nodes which have the same cost as per the offline version heuristic. The heuristic used for this problem is given below $$h_{1}(n) = \sqrt{SLD(g, n) * \bar{h}(n)}$$ where SLD(g, n) stands for the short line distance between the current node and the goal node. In general, I also proposed a family of heuristics defined as given below $$h_k(n) = = \sqrt[k+1]{{\bar{h}(n)}^{k} * SLD(g, n)}$$ With increasing k, the computation time for the heuristic increases but with improved performance. In particular, as k \(\to \infty , h_{k}(n) \to \bar{h}(n)\), which is the true cost in an obstacle free world. The primary intuition behind the formulation of this family of heuristics is that multiplication by SLD brings in a way within the heuristic to prefer nodes which have shorter SLD when their \(\bar{h}(n)\) costs are tied, which turns out to be very useful and taking the k+1 th root just ensures that the heuristic is admissible. More intuition, details, proofs about admissibility are presented in the report. </li> </ul><br><br><h4> PID controller</h4> The equation of a PID controller is given below <br>.$$\bar{u}(t) = K_p e(t) + K_I \int_{0}^{t} e(\tau) d\tau + K_D \frac{de(t)}{dt}$$. I implemented a discrete version of PID control for angular velocity.$$\bar{\omega}(t) = K_{wp}  e(t) + K_{wi}\sum_{i=0}^{i=t} e(t) \Delta_t + K_{wd} \frac{e(t-1) - e(t)}{\Delta_t}$$. For linear velocity, I just a implemented a proportional controller.<br> $$\bar{v}(t) = K_{vp} * \sqrt{{(\frac{\Delta_x}{\Delta_t})}^{2} + {(\frac{\Delta_y}{\Delta_t})}^{2}}$$<br><br> You can view the <a href="./files/astar.pdf" target="_blank">report</a> here.<br><br> You can visit the <a href="https://github.com/senthilpalanisamy/astar_planner" target="_blank">github repo</a> for viewing code.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-3" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Object Manipulation using Youbot in Simulation.</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/youbot.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Overview</h2>  As the final project for the course on Robotic Manipulation, I implemented a PI feedforward controller for a 4 wheeled mobile robot with a 5 dof arm for an object manipulation task. The arm has 5 degrees of freedom and the wheels add 4 degrees of freedom making this a redundant robot with 9 degrees of freedom. The PI feed forward controller contains three terms $$ V(t) = [Ad_{X^{-1}X_d}] V_d(t) + K_{p} X_{err}(t) + K_{i} \int_{0}^{t} X_{err}(t) dt.$$<ul> <li> The feed forward term \([Ad_{X^{-1}X_d}] V_d(t)\)  which generates the twist for following a desired trajectory. (V_d is the twist required to take current desired end effector configuration to the next desired end effector configuration and is given by \([V_d] = (1 / \Delta t) log(X_d^{-1} X_{d, next})\) . This twist is then converted from the current desired end endeffector frame to the actual current end effector frame</li> <li> An error term which react to the error of the end effector \(K_{p} X_{err}(t)\) (the twist required to take the current end effector configuration to a desired end effector configuration at the current time step) </li> <li> An integral term which keeps a running sum of the errors accumulated so far.\(K_{i} \int_{0}^{t} X_{err}(t) dt\) ( summation of the error twists over time )</li> </ul> The output twist obtained by summing all three terms is then converted to robot velocity commands by using the Jacobian matrix associated with the 9D robot. \(\begin{pmatrix} u \\ \dot{\theta} \end{pmatrix} = J_e^{+} V.\)<br><br> Then the robot configuration is estimated by the applying the command velocities through the kinematic model of youbot.  <h2> A few observations</h2> The kerms Kp and Ki control how the system behaves <ul> <li> A high value of Kp brings down the errors rapidly width="400".</li> <li> A low value of Kp just makes the system’s response to errors slower.</li> <li> But a very high value of Kp causes jerky motion in the robot. </li>  <li> Ki is helpful for tracking problems for achieving absolute and quick convergence to the desired values but they sometimes induce oscillations and overshoots. Hence, this value is kept very low. </li> </ul>  <h2> Some minor algo details.</h2> For this problem, I needed to set wheel radius to 1.5 times of the normal wheel radius ( 1.5 * 0.0475 = 0.07125m ) to achieve a satisfactory grasping. A kp value of 1.5 and ki value of 0 was found to give satisfactory results and drove the errors to zero quickly without inducing overshoots.<br><br> <img src="./img/portfolio/youbot_error.png" class="center" width="560"> <br><br> When the same kp values were tried for the new tasks, the video looked good and the error graph looked almost similar. <div style="display:flex"> <div style="flex:1;padding-right:5px;"> <img src="./img/portfolio/youbot_newtask.png" width="560" class="left"> </div> <div style="flex:1;padding-left:5px;"> <iframe width="560" height="315" src="https://www.youtube.com/embed/FF0jhb3Z0xs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> </div> </div> If very higher Ki values are used, overshoots were observed and in some cases the error didn't converge to zero before pickup and hence the pikcup failed. When \(K_i\) value of 0.25 and \(K_p\) value of 1.0 was used, the following error pattern and manipulation behavior was observed.<div style="display:flex"> <div style="flex:1;padding-right:5px;"> <img src="./img/portfolio/youbot_overshoot.png" width="560" class="left"> </div> <div style="flex:1;padding-left:5px;"> <iframe width="560" height="315" src="https://www.youtube.com/embed/KiBffqOmgKQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>> </div> </div><br><br> <h2> Joint Limits </h2> Sometimes the robot involves in self collision like some in the video below. <iframe width="560" height="315" src="https://www.youtube.com/embed/Fl1smOTt_Xw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <br><br> To avoid this, I also implemented joint limits  so that self collisions are avoided. I also implemented singularity checks so that the arm configuration never goes near singulairy. The limits identified for the joints to achieve this objective were as follows<ul><li> \(\theta_1\) should be between -1.65 to 1.65 ( this limits avoid the arm to go and hit the back of the body or its sides)</li> <li> \(\theta_2\) should be less than -0.85 (This limit ensures that the arm is stretched out and hence reduces chances of collision) </li> <li> \(\theta_3\) and \(\theta_4\) are constrained to be less than -0.2 so that the arm doesn’t get close to a singularity.</li></ul> The robot was able to do good pickup with these limits but I did notice slow convergence of errors and sometimes, very small residual errors remained. This should be because my joint limit values should have been a touch conservative, which explains constant but very small errors. The video showing outputs of joint limits in action is shown below <iframe width="560" height="315" src="https://www.youtube.com/embed/WWke3m4Hq_g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br> This video clearly shows that robot avoids self collision using the joint limits yet performs useful pick and place activities.<br><br> Due to copyright issues, the code of this project cannot be shared in public. If you like to discuss anything on the implementation, feel free to contact me.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-4" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>OCV (Optical Character Verification) using smart camera</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/ocv.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description </h2> This project was done as a proof of concept for building a software that verifies if the labels are correctly printed. It should be noted that the verification problem is much simpler version than the recognition problem, where the real question we are asked is to find out what character are present in the given image. But in verification problem, the real question we are being asked is "are these characters same as the characters I showed you previously?". This simplification of verification allows us to construct simple algorithm which do not necessarily understand what characters are present in the image but can verify if the given characters are the same as the characters registered in a template.<br> <h2> Algorithm Description </h2> <ul> <li> A template image is first registered. Key points are detected on the template image and descriptors are built for each key point detected. The key point detectors and descriptors used in this work are the ORB feature descriptors. There are more powerful feature descriptors like SIFT but they are not commercial license friendly. </li> <li> A new query image is given for verification. key points are detected on the query image and key point descriptors are built around each detected key point. Once again, ORB feature descriptors are used in this case</li><li> The key point descriptors from query image are matched against key point descriptors from template image and the matching key point pairs are found. Lowe's ratio was used to remove ambiguous matches in this step</li><li> Finally, a transformation is computed using the matching key point pairs. This transformation aligns the query image to be in the same pose as the template image. The transformation computed here is the Euclidean transformation since it only involves rotation and translation. But if other variations such as scaling, skew, perspectivity are allowed, one should consider more general transformations, with homography being the top of them all</li><li> Finally, after alignment both template image and query image are binarized using otsu's thresholding. Otsu's thresholding is useful in this case since there are only two colors, the foreground and the background. Then difference between the query image and the template image is computed for registered ROI </li><li> If the difference image between the two is all zeros, the registered text is present in the given query image and hence, it is declared as a "pass". If the difference image is not all zeros, then it means that the registered text is not present and hence, it is declared as a "Failure" </li><li> Sometimes, due to small noises, the difference image will not be all zeros. To avoid this, morphological closing is applied before checking for the difference</li><li> If the alignment fails due to lack of key point matches, it also indicates a failure since that can only happen if the template and query image are completely different. </li></ul><h2> Some tips for speedup</h2> This algorithm took 90 ms on a cortex A9 processor. The main design decision that made this processing time possible were <li> Choose the right transforms. A homography might be a very tempting choice since its a very general transform that can handle all variations but using a euclidean transform instead of a homography after carefully studying the problem constraints cut 50 ms of processing time </li> <li> I cut down the pyramid construction in ORB feature generation since scale invariance was not a necessary demand as per problem specification. <h2> Challanges in working with ARM Processors</h2> OpenCv has been around for long and it has been well optimized for Intel chips since the SIMD instruction set for Intel which is SSE is well written for OpenCV, thus exploiting any parallel resources available. But same is not true for ARM chipset. The analogous SIMD instruction for ARM which is NEON is not fully written at the low level for OpenCV, thus missing to exploit available parallel resources. Therefore, if we were to compare the same algorithm running on a quadcore intel and ARM chipset with very similar computing powers, on an average 1:4 slowdown was observed with ARM chipset. At the time of publishing this post (December 2019), there isn't a great effort seen in OpenCV to make it Neon optimised and hence, this slowdown trend is likely to continue for years to come with OpenCV.<br><br> The whole algorithm is clearly explained in the video along with a demo <iframe width="560" height="315" src="https://www.youtube.com/embed/omvuAs_Ha4s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br> Due to copyrights, I cannot share the code I wrote for my company. Feel free to get in touch with me, if you have doubts in any part of the algorithm.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2017</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-5" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Seat Belt Detection</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/seatbelt.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description </h2> This project was done as a proof of concept for monitoring if drivers always wear seat belt in lorries. I trained machine learning based object detector for achieving this objective. These algorithms were not coded from scratch. They were coded using OpenCV and SKlean libraries.<br><br> <h2>Algorithm Description</h2><ul><li>This is a sliding window detector that classifies each window using a SVM classifier trained on HoG features</li><li>To generate training set, 2000 image frames were extracted from a video and each frame was annotated with a Ground Truth bounding box of Seat Belt.</li><li> Then a viewing window size was defined and all possible windows were generated by sliding window mechanism. For each window HoG (Histogram of Oriented Gradients) were generated and given a Supervised label of "Seat Belt Detected" or "No Seat Belt Detected". All windows which have at least 90% IoU (Intersection Over Union) with the ground truth bounding box annotation of seat belt were given "Seat Belt Detected" label while others were given "NO Seat Belt Detected" label</li><li> After the first set of training, to improve the performance further, hard negative mining was performed (Where many mis-classified were added back to the training dataset)</li> <li> To improve the quality of the existing data, data augmentation was performed. It must be noted that the type of image augmentations chosen should be thought along with the actual problem in hand. Sometimes, we may end up making the problem more difficult than it actually is by introducing variations that are unlikely to occur in the real use case. This will, in fact bring down the performance from the optimal performance which could have been achieved with carefully chosen variations that fit the problem description </li><li> Finally, since we will end up getting multiple detection, I ended up doing Non-Maximum suppression, where multiple local detection are merged into a one peak maxima detection.</li><li> The final IoU of the detector turned out to be 75%</li></ul><br><br>. A few more videos showing outputs are shown below.<iframe width="560" height="315" src="https://www.youtube.com/embed/42qybhjpUBk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/4IZcE65EVds" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/23CCyuAoDe8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2017</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-6" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Aadhar Card reader</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/card_reader.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description </h2> Card readers are becoming a versatile application in India and this is one such project to read one of India's government Identification cards, which is called the Aadhar. The first idea was to extend this to a lot more Indian government Identification cards like PAN, Passport and Voter Id but we didn't have the time to extend it but this algorithm works well for Aadhar card. <h2> Algorithm Description</h2><ul><li> The four corners of the card is first found by detecting all the edges of the card using canny edge detection. It must be noted that a clear contrast between the card and its background is assumed for this step. In order to make it more robust, more complex detection algorithms should be used. </li><li> A template view of the card is defined based on predefined 2D pose (The card looks absolutely horizontal with no rotation). Now a homography is estimated to align the given card to the predefined 2D pose.</li><li> Now the hhomography is applied to transform the card to its template view, where all text is now horizontal. Now, SWT (Stroke Width Transform) is applied to detect all text in the image.</li><li> After detecting text, the text is segmented into characters and each character is then recognized using Deep Learning OCR model that we trained. This model was originally trained on 0.1 Million images of handwritten character but this model also offered great performance for printed characters as well</li><li> Tesseract Engine can also be used for segmentation and character recognition but many underlying parameters have to carefully fine tuned to get it working to a decent level. But overall, we got better performance with our HCR model on these printed character than the Tesseract Engine on these printed characters. With Tesseract, the accuracy was about 87% while with our custom trained model, the accuracy was about 96%.</li></ul> The demo can be accessed in <a href="https://digitizekyc.herokuapp.com/" target-"_blank">this</a>link.<br><br> Since this a work that did for my company, the code cannot be open sourced but feel free to get in touch with me to know more about this.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2017</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-7" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Magic book</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/magic_book.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description </h2>  This is a project I worked in Soliton, where we deliver R&D as a service. Due to Non-Disclosure Agreements we had with the customer, I had cannot reveal any part of the algorithm design. I am just going to state what part I worked in this product, when delivered some parts of this software as a service. <br><br> <h2> My role in this project</h2><ul><li> I was a part of the team that developed an algorithm that synthesizes adaptive background for a given album page by picking up colors from all the photos in the page. This was done by Filtering colors according to a definition (which cannot be revealed, because that's propriety of the company)and quantiszed them to pick colors and used them to synthesize a  background. </li><li>Developed an algorithm for placing decoration objects according to the rules defined by the aesthetics associated with each of them. Came up with a mathematical formulation to measure the aesthetics and storytelling value of a picture.<br><br> For more details on the product, visit the <a href="https://www.gomagicbook.com/" target="_blank">product</a> website. This is one of those rare projects, where I am going to state this - " Please don't get in touch with me if you need to know more on this because I cannot reveal more than what I have written here"</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2017</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-8" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Rapidly Exploring Random Tree</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/rrt.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description </h2>  RRT is a fundamental planning algortihm in robotics and it is very useful especially when in planning in high dimensional non-convex spaces where discreting the whole space and planning exhaustively based all discrete point is not feasible.<br><br><h2> Algorithm Description</h2><ul><li> A rrt begins by first selecting a sampling a point randomly and cheks if the point is not inside any obstacles</li> Then the node which is the nearest node to the sampled point is chosen and then a unit vector pointing the direction chosing the sampled point and nearest node on the graph is calculated</li><li> Then, the graph is expanded \(\Delta\) distance in the unit vector direction from the nearest node provided the whole extension does not collide with any obstacles. A pesudocode of the whole algorithm is shown below<br><br> <img src="./img/portfolio/rrt_pseudocode.png" width="560" class="center"></li> </li>The termination point of the algorithm is when there is a direct obstacle free path from the one of the nodes in the graph to the goal destination.<li> A point which needs further eloboration from the above description is how do we check for collision during a node expansion. This checking becomes simplified when all obstacles are circular because all we need to check are only two things<ul><li>The prependicular distance of the line to be extended from each of the circle center should be less than their respective circle radius</li><li> The end points of the line should not line inside any of the circle</li></ul>Such a simplified implemenation is shown below.<br><br><img src="./img/portfolio/circular_rrt.gif" width="560" class="center"></li><li> But when we are given random obstacles, we should then use Bresnan's line algorithm to check for collisions all along the line. This is the implementation that is shown at the top of the post.</li></ul> The code for this implementation can be found at one of <a href='https://github.com/senthilpalanisamy/msr_assignments' target="_blank">my github repos</a><br></p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2017</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-9" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Servo Controlled Ball tracking</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/balltracker.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description</h2> This project was done during MSR hackathon. The objective was to assemble and control a two Degree of Freedom Pan-tilt mechanism that uses comptuer vision for tracking and following a Blue cube<br><br><h2>Detailed Overview</h2><ul><li>As a first step, I assembled the two servos into a pan-tilt mechanism</li><li>Next, after soldering the Pololu Micro-maestro board, I installed its drivers and wrote python wrappers that allowed me to control the pan-tilt servos by specifying a trajectory sequence of angles for both servos</li><li> Finally, to achieve the vision based control, I wrote a OpenCV based tracker. Initally, the blue cube is found by masking the blue color and filtering contours based on pre-defined contour properties. Once the cube is detected, it is then tracked using a CSRT tracker through successive frames. The center of the contour defining the blue cube is used for achieving the control. The pixel disparity of the contour center from the image center is calculated in both X and Y direction. The angle of view of the camera was calculated and then the pixel disparity was conveted to proportional angle disparity and this angle was then used a feedback to adjust the pan and tilt servo until the ball was nearly at the center of the image</li><li> One complexity that is worth mentioning is the need to invert camera image when the tilt swithces from positive to negative values so that the servo control can be the same at all pan-tilt configurations</li></ul> A video showing the whole output is shown below <iframe width="560" height="315" src="https://www.youtube.com/embed/3hftMyb5vug" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br> Feel free to check out the <a href="https://github.com/senthilpalanisamy/msr_assignments" target="_blank">code</a> in github.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">August 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-10" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Other Computer Vision projects</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/computer_vision.jpg" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief overview</h2> There are a lot of short term projects I worked in my previous work experience, for which I cannot showcase the work outside due to the non-disclosure agreement involvement in those works. Nevertheless, I am briefly stating the work experience I had in each of those projects<br><br><h2>Deep Learning based projects</h2> In these ages, a computer vision Engineer doesn't exist without knowing deep learning. I worked on a couple of deep learning based porjects<br><br><h4>Image Depth Categorisation based on deep learning</h4>Images have to be classified into one of four categories- Close-up, Medium, Long and Ultralong range shot. I generated depth maps for a given image based on pre-trained monocular depth estimation models and constructed a four channel RGBD image by appending the depth map to an RGB image. Then I trained different existing dDeep Learning architectures on 4D images and fine-tuned a model based on ResNet-50 to achieve an accuracy of 85%.<h4>Weed detection and distance estimation using calibrated camera</h4> Trained a weed detection model using tensorflow object detection api. Calibrated the extrinsic parameters of the rover camera with respect to a checker board placed on ground. Found the 3D point that lies at the intersection of ground plane and inverse projection ray (that corresponds to the bottom most pixel of weed). Estimated the distance of the rover to the point using known transformations.<br><h2> Classical Vision Projects</h2><br><h4> Shape Context matching as a post-processing to improve Deep Learning HCR accuracy</h4> This is not a real deep learning project but an effort to improve the DL model performance.I improved the accuracy of an AlexNet HCR model from 90.1% to 91.2% by overriding deep learning outputs by shape context-based matching results only for images where the deep learning network was struggling to classify due to very low confidence.In order to achieve this, I had a database of hand written characters to compare against and whenever the deep learning model was not confident enough, I overrode deep learning predictions with Shape context based prediction. To achieve this,  I measured the Hausdorff distance between the shape context approximation of the given image and shape context approximation of every image in the database and found the best match (The image with the lowest hausdroff distance with using shape context features.) I Sped up this computation by 3x using parallel processing.<br><br><h4>Improving Casting and Bagging Automation system</h4>This is a Jewelry component identification system consisting of 85,000 different Jewellery parts. The existing pipeline that was in action consisted of a size filter(filtering based on contour area), hu moments filter followed by pattern matching.To improve the overall system accuracy, I proposed two changes based on ablative analysis- Using Zernike moments instead of Hu and ORB feature matching instead of pattern matching improved the overall recognition accuracy from 55% to 80%.<h4> Iris recogniton</h4>Developed an end to end iris recognition system involving image acquisition through Soliton smart camera, Iris localization by Daugman’s transform, image normalization and unwrapping, encoding by Gabor wavelets and matching by L2 distance. I didn't really develop any of these algorithms from sratch in this case. I just integrated all the open source code to get a system working. <br><br><h2>Tools/ Non-CV project</h2><h4> Dataset Generator</h4> The objective behind this project was to develop a tool that will lower the human effort in data collection. The basic idea was to collect images and auto-tag some of them using on-the-fly machine learning models. Thus this tool runs a weak model automatically on the labeled data collected so far and auto-tags some images within the dataset, which it believes with extreme confidence.<br><br><h4> Pin Map Generation</h4> The objective in this project was to find a pin mapping between two devices based on netlist connection given. To do this, I generated a graph representing circuit connections for a given netlist. Then I found the pin mapping between any two terminals  present in the circuit by applying shortest path algorithms on the generated graph. This problem setup is challenging particularly because the graph nodes are not simple nodes but electronic components. So electronic circuit knowledge needed to be incorporated to define connectivity between nodes.<br><br></p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">Nov 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-11" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Workshops Conducted</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/workshop.jpg" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2> Brief Description</h2> I, along with my team in Soliton Conducted three workshops on Computer Vision between 2016-19. I have listed the ppts, link to github repo and the parts I handled for each workshop here. Over the years, I have handled Machine learning and Deep learning in the context of computer vision and fundamentals of projective geometry in these workshops<br><br><h2> Anthill Workshop 2018</h2><ul<li> Anthill is an annual machine learning conference organised in Bangalore. We presented a 2 Day workshop on "DL and ML for computer Vision" at this conference. Link to <a href="https://anthillinside.in/2018-computer-vision-workshop/" target="_blank">the workshop</a></li><li> Link to <a href="https://github.com/dhivakark/ahws_18" target="_blank">the github repo</a></li><li> You can view <a href="./files/Day1_PPT.pdf" target="_blank">the day1 PPT</a> and <a href="./files/Day2_PPT.pdf" target="_blank"> day2 PPT2</a> here.</li><li> I handled the machine learning part of the workshop.<h2> PSG Workshop 2018</h2> PSG is a reputed college in South India. We were invited to give a two day workshop for the final year undergraduate students and faculty at PSG. Link to the <a href="http://www.psgtech.edu/Teqip_Workshop%20Feb_2018.pdf" target="_blank">workshop decription</a><ul><li>Link to <a href="https://github.com/dhivakark/psg_ws_18" target+"_blank"> github repo</a> You can view the <a href="./files/PSG_Day1.pdf" target="_blank"> Day-1 PPT</a> and <a href="./files/PSG_Day2.pdf" target="_blank"> Day-2 PPT</a> here</li><li> I handled the projective geometry and deep learning part of the workshop.</li></ul><h2>Anthill Workshop 2017</h2><ul<li> Anthill is an annual machine learning conference organised in Bangalore. We presented a one Day workshop on "DL and ML for computer Vision" at this conference. Link to <a href="https://fifthelephant.in/2017-aug-dl-and-ml-for-cv-workshop/" target="_blank">the workshop</a></li><li> Link to <a href="https://github.com/dhivakark/ahws_17" target="_blank">the github repo</a></li><li> The PPT used for this workshop is just a subset of the PPT used in 2018 workshop</li><li> I handled the whole deep learning part of the workshop.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">Nov 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-12" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Dynamic simulation of a windmill impact</h2>
                            <hr class="star-primary">
                            <img src="img/portfolio/windmill_impact.gif" class="img-responsive img-centered" alt="image-alt">
                            
                                <p><h2>Breif Overview</h2> A triangular mass falling in gravity constrained to move in a circular path goes in impact with a rotating windmill.This system consists of two independent rigid bodies.<ul><li>The rotating windmill is a single DoF body consisting of three rectangular plates joined at the center of one of the edges.</li><li> The triangular mass is a 3 DoF system and it is free falling in gravity but it is constrained to move in a circle.</li></ul> The impact of windmill on the traingular mass is modelled but the impact of triangular mass on the turbine is not modelled. The turbine is modelled to be a very heavy body and triangular mass is much smaller when compared to turbine so that this assumption is meaningful.Therefore,  the trainagle doesn't  cause any changes through impact to the dynamics of the turbine. But the dynamics of the triangular mass is altered through impacts with the windmill and that is the heart of the project.<h2> Frames setup</h2> A figure showing all frames used in the project is shown below<br><br><img src="./img/portfolio/dynamics_frames.jpeg" width=560 class="center"><br> Each rectangle in the windmill has a frame and its transformation is given in the figure. The windmill as a whole is a one DoF system and is pivoted at the point X=5, Y=5. The angle between each of the rectangle is 120 degrees. The triangular mass is a 3 DoF system and it has direct transformation characterised by x,y and \(\theta_2\) with respect to the world frame. The triangle is assumed to be equilateral. Not all of these frames are important in lagrangian computation. Some of these frames have been specifically setup for handling impact conditions. Its very easy to define phi conditions for impact if we setup a frame that is aligned with each of the sdies of impact. Frames s1, s2, s3, b1, b2, b3 are used in writing down the equation and limits of impact phi surfaces while b1c, b2c, b3c and t are useful in writing down the lagrangean of the system <h2> Lagrangian setup</h2> The Lagrangrean for the system consits of writing down potential and kinetic energies of the four bodies (three rectangles and one triangle). Gravity is pointing in the negative y direction. So the lagrangian consists of 4 PEs and 4 KEs, one each for each of shapes (3 rectangles and 1 triangle)<br><br> <h4>Number of degrees of freedom</h4> 4 (\(\theta_1, \theta_2, x, y)\)<br><br><h4>Constraints</h4>The triangle is contrained to move in a circle. This circle is centered at (5, 13) and has a radius of 5<br><br><h4>Impacts</h4>Turbine impacts the triangular mass. In order to achieve this, I had to keep track of 12 edges for the phi surface (9 edges of rectangle + 3 edges of triangle, the pivoted edges of rectangles are ignored because they never involve in impact) and 9 points ( 2 points from each rectangle and 3 points from triangle, the two points on the pivoted edges of the rectangle don't involve in impact). In all I had to check for about 45 impact conditions but I should downplay this exaggeated number by stating that all of them revolve around just 2 or 3 key ideas. The impact update law used is for this work is shown below.<br><br>\(-[H]^{\tau+}_{\tau-}=0\)<br><br>\(\frac{\partial L}{d\dot{q}}{\bigg\rvert}^{\tau+}_{\tau-}  = \lambda_I \nabla\phi_I(\epsilon)+ \lambda_p \nabla\phi_p(q)\)<br><br>\({\frac{d}{dt} \phi_p(q)}{\bigg\rvert}^{\tau+}_{\tau-} = 0\)<br><br> I have used two lambda terms  \(\lambda_I\) and  \(\lambda_p\) corresponding to two constraints on the generalised moementum after impact in  the impact update, because I needed the path constraint for the trianglar mass to be upheld even after impact.<br><br> <h4>External force / Torque</h4> The windmill is driven by a external torque. This torque is applied to the \(\theta_1\) when solving EL equations of the system.<br><br> Feel free to check out the code in <a href="https://colab.research.google.com/drive/1wBqgbCS2MAZy1PqR2_w96NRRAnoMrOx3" target="_blank">colab</a>.</p>
                            
                            <ul class="list-inline item-details">
                                
                                
                                    <li>Date:
                                        <strong><a href="http://startbootstrap.com">December 2019</a>
                                        </strong>
                                    </li>
                                
                                
                            </ul>
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

     <!-- jQuery Version 1.11.0 -->
    <script src="/js/jquery-1.11.0.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="/js/jquery.easing.min.js"></script>
    <script src="/js/classie.js"></script>
    <script src="/js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="/js/jqBootstrapValidation.js"></script>
    
    <script src="/js/contact_me_static.js"></script>
    

    <!-- Custom Theme JavaScript -->
    <script src="/js/freelancer.js"></script>

    

    </body>
</html>
