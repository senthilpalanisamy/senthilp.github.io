---
layout: default
modal-id: 1
date: 2019-12-23
img: baxterbuilder.gif
alt: image-alt
project-date: December 2019
description:  <h2>My contribution in this team project</h2> I implemented the whole node for computer vision, which detects AR blocks, red lego blocks and does inverse projection to find the 3D location of a point on a known plane corresponding to a pixel on the camera. I also setup the RoS pipeline for the whole project and integrated my node into our RoS pipeline <br><br><h2> Overview </h2> For this project, Baxter assembles a MEGA BLOKS pyramid from the blocks provided by the user. The flow is:<ul><li>The Blocks that can be picked up are detected on Baxter's camera image using OpenCV</li><li>Among all detected blocks, a block is chosen at random and its 3D location is estimated</li><li>The block is picked up and moved to the drop location using position control in Moveit</li><li> The block is pressed against the plate by using force control in Moveit and finally, as a double check, the gripper hand is pushed against the block to ensure that the block is pushed in.</ul><br><br><h2>detailed explanation</h2> <br><br><h4>computer vision</h4>We used the Baxter's right hand to observe the base plate, spot a red brick and determine its position for pickup. First to know the plane in which the base plate is located, all AR frames visible in the camera's field of view are detected. Then the pre-calibrated intrinsic camera parameters are used to find inverse projection ray corresponding to a pixel on the camera's sensor. Then the 3D location corresponding to a pixel is calculated by finding the intersection of the inverse projected ray and ground plane defined by one of AR tags. The 3D point corresponding to the pixel is determined in multiple AR frames and then these points are converted to the Baxter world frame representation. As a result, we get multiple points in world frame that correspond to the same pixel in Baxter's camera image but obtained using different AR frames as reference. The median of all those points is found out and that point is taken to be the true 3D location corresponding to the pixel in the image. Reason why we use multiple AR frames when one should be sufficient in theory - Sometimes AR tags are affected by lighting and their 3D pose with respect to the camera oscillates. Even worse sometimes, they are not detected. Using multiple AR tags this way increases both the roboustness and the precision of the system. We were able to locate points within +/- 1 cm accuracy through this process.<br><br> <h4>Pickup and Placning</h4>We then use move it to move Baxter's left arm to the block pickup location, pickup the block by controlling gripper through Baxter's native libraries and then use move it again to move the blocks to a pre-configured goal location. Position control was used to move Baxter's arm to pickup and drop locations. We had standoff points before the pickup and drop, that were designed to be a few cms above the actual pickup and drop location. When placing the block, we used force control to press the block against the plate. The whole process is repeated until 3 blocks are placed to construct a small pyramid. A video showing this whole process in action is shown below but it should be noted that the video has been sped up 3x for viewing convenience <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/mz1FwBR94og" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br><br> If you want to know more on the project, please check out the <a href="https://github.com/senthilpalanisamy/final-project-megabloks" target="_blank">code</a> in github.
title: Baxter, the lego-builder 

---
